#!/cvmfs/oasis.opensciencegrid.org/ligo/sw/pycbc/x86_64_rhel_7/virtualenv/pycbc-v1.16.13/bin/python

# Copyright 2016 Thomas Dent
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.

from __future__ import division

import sys, h5py
import argparse, logging

import copy, numpy as np
import time as tt
from pycbc import io, events
from pycbc.events import triggers, trigger_fits as trstats
from pycbc.events.stat import sngl_statistic_dict
import pycbc.version
from pycbc.events import triggers
import pycbc.conversions as conv
import pycbc.pnutils as pnu


#### DEFINITIONS AND FUNCTIONS ####
def get_stat(statchoice, trigs, threshold):
    # Initialize statclass with an empty file list. In general could feed it
    # files here for statistics which need that.
    stat_instance = sngl_statistic_dict[statchoice]([])
    chunk_size = 2**23
    stat = []
    select = []
    size = len(trigs['end_time'])
    s = 0
    while s < size:
        e = s + chunk_size if (s + chunk_size) <= size else size

        # read and format chunk of data so it can be read by key
        # as the stat classes expect.
        chunk = {k: trigs[k][s:e] for k in trigs if len(trigs[k]) == size}
        chunk_stat = stat_instance.single(chunk)
        above = chunk_stat >= threshold
        stat.append(chunk_stat[above])
        select.append(above)
        s += chunk_size

    # Return boolean area that selects the triggers above threshold
    # along with the stat values above threshold
    return np.concatenate(select), np.concatenate(stat)


# MAIN
parser = argparse.ArgumentParser(usage="",
    description="Smooth (regress) the dependence of coefficients describing "
                "single-ifo background trigger distributions on a template "
                "parameter, to suppress random noise in the resulting "
                "background model.")

parser.add_argument("--version", action=pycbc.version.Version)
parser.add_argument("-V", "--verbose", action="store_true",
                    help="Print extra debugging information", default=False)
parser.add_argument("--veto-file", nargs='*', default=[], action='append',
                    help="File(s) in .xml format with veto segments to apply "
                    "to triggers before fitting")
parser.add_argument("--veto-segment-name", nargs='*', default=[], action='append',
                    help="Name(s) of veto segments to apply. Optional, if not "
                    "given all segments for a given ifo will be used")
parser.add_argument("--prune-param",
                    help="Parameter to define bins for 'pruning' loud triggers"
                    " to make the fit insensitive to signals and outliers. "
                    "Choose from mchirp, mtotal, template_duration or a named "
                    "frequency cutoff in pnutils or a frequency function in "
                    "LALSimulation")
parser.add_argument("--prune-bins", type=int,
                    help="Number of bins to divide bank into when pruning")
parser.add_argument("--prune-number", type=int,
                    help="Number of loudest events to prune in each bin")
parser.add_argument("--ifo", required=True,
                    help="Ifo producing triggers to be fitted. Required")
parser.add_argument("--log-prune-param", action='store_true',
                    help="Bin in the log of prune-param")
parser.add_argument("--template-fraction-range", default="0/1",
                    help="Optional, analyze only part of template bank. "
                    "Format is PART/NUM_PARTS")
parser.add_argument("--fit-function",
                    choices=["exponential", "rayleigh", "power"],
                    help="Functional form for the maximum likelihood fit")
parser.add_argument("--f-lower", type=float, default=0.,
                    help="Starting frequency for calculating template "
                         "duration, if not reading from the template fit file")
#parser.add_argument("--log-param", nargs='+',
#                    help="Take the log of the fit param before smoothing.")
#parser.add_argument("--smoothing-width", type=float, nargs='+', required=True,
#                    help="Distance in the space of fit param values (or the "
#                         "logs of them) to smooth over. Required. "
#                         "This must be a list corresponding to the smoothing "
#                         "parameters.")
parser.add_argument("--fit-param", nargs='+',
                    help="Parameter(s) over which to regress the background "
                         "fit coefficients. Required. Either read from "
                         "template fit file or choose from mchirp, mtotal, "
                         "chi_eff, eta, tau_0, tau_3, template_duration, "
                         "a frequency cutoff in pnutils or a frequency function"
                         "in LALSimulation. To regress the background over "
                         "multiple parameters, provide them as a list.")
parser.add_argument("--min-duration", type=float, default=0.,
                    help="Fudge factor for templates with tiny or negative "
                         "values of template_duration: add to duration values"
                         " before fitting. Units seconds.")

parser.add_argument("--save-trig-param",
                    help="For each template, save a parameter value read from "
                    "its trigger(s). Ex. template_duration")

parser.add_argument("--fit-stage1",
                    help="hdf5 file containing fit coefficients for each"
                         " individual template from stage1. Required")

parser.add_argument("--trigger-file",
                    help="hdf5 file containing trigger distribution"
                         ". Required")

parser.add_argument("--connection-file",
                    help="hdf5 file containing coarse-fine connection template ids"
                         ". Required")

parser.add_argument("--sngl-stat", default="new_snr",
                    choices=sngl_statistic_dict.keys(),
                    help="Function of SNR and chisq to perform fits with")
parser.add_argument("--stat-threshold", type=float,
                    help="Only fit triggers with statistic value above this "
                    "threshold.  Required.  Typically 6-6.5")

parser.add_argument("--coarse-bank-file", default=None,
                    help="hdf file containing template parameters. Required "
                         "unless reading param from template fit file")

parser.add_argument("--bank-file", default=None,
                    help="hdf file containing template parameters. Required "
                         "unless reading param from template fit file")

parser.add_argument("--output", required=True,
                    help="Location for output file containing smoothed fit "
                         "coefficients.  Required")


args = parser.parse_args()

pycbc.init_logging(args.verbose)

args.veto_segment_name = sum(args.veto_segment_name, [])
args.veto_file = sum(args.veto_file, [])

if len(args.veto_segment_name) != len(args.veto_file):
    raise RuntimeError("Number of veto files much match veto file names")

if (args.prune_param or args.prune_bins or args.prune_number) and not \
   (args.prune_param and args.prune_bins and args.prune_number):
    raise RuntimeError("To prune, need to specify param, number of bins and "
                       "nonzero number to prune in each bin!")

if args.verbose:
    log_level = logging.DEBUG
else:
    log_level = logging.WARN
logging.basicConfig(format='%(asctime)s : %(message)s', level=log_level)

logging.info('Fitting above threshold %f' % args.stat_threshold)


logging.info('Opening stage-1 fit file: %s' % args.fit_stage1)
fit_stg1 = h5py.File(args.fit_stage1, 'r')

logging.info('Opening trigger file: %s' % args.trigger_file)
trigf = h5py.File(args.trigger_file, 'r')

logging.info('Opening template file: %s' % args.bank_file)
templatef = h5py.File(args.bank_file, 'r')

#num_templates = len(templatef['template_hash'])
#tmin, tmax = parse_template_range(num_templates, args.template_fraction_range)
#logging.info('Analyzing template %s - %s' % (tmin, tmax-1))

logging.info('Opening connection file: %s' % args.connection_file)
confile = h5py.File(args.connection_file,'r')

logging.info('Opening coarse bank file: %s' % args.coarse_bank_file)
coarsebank = h5py.File(args.coarse_bank_file,'r')

logging.info('Counting number of triggers in each template')
# template boundaries dataset is in order of template_id
tb = trigf[args.ifo+'/template_boundaries'][:]
tid = np.arange(len(tb))
# template boundary values ascend in the same order as template hash
# hence sort by hash
hash_sort = np.argsort(templatef['template_hash'][:])
tb_hashorder = tb[hash_sort]
# reorder template IDs in parallel to the boundary values
tid_hashorder = tid[hash_sort]

# Calculate the differences between the boundary indices to get the
# number in each template
# adding on total number at the end to get number in the last template
total_number = len(trigf[args.ifo + '/template_id'])
count_in_template_hashorder = np.diff(np.append(tb_hashorder, total_number))
# re-reorder values from hash order to tid order
tid_sort = np.argsort(tid_hashorder)
count_in_template = count_in_template_hashorder[tid_sort]

# get the stat values
logging.info('Calculating stat values')
abovethresh, stat = get_stat(args.sngl_stat, trigf[args.ifo],
                                 args.stat_threshold)
tid = trigf[args.ifo + '/template_id'][abovethresh]
time = trigf[args.ifo + '/end_time'][abovethresh]

if args.save_trig_param:
    tparam = trigf[args.ifo + '/' + args.save_trig_param][abovethresh]
logging.info('%i trigs left after thresholding' % len(stat))

# Calculate total time being analysed from segments
# use set() to eliminate duplicates
segment_starts = sorted(set(trigf['{}/search/start_time'.format(args.ifo)][:]))
segment_ends = sorted(set(trigf['{}/search/end_time'.format(args.ifo)][:]))
all_segments = events.veto.start_end_to_segments(segment_starts, segment_ends)

# now do vetoing
for veto_file, veto_segment_name in zip(args.veto_file, args.veto_segment_name):
    retain, junk = events.veto.indices_outside_segments(time, [veto_file],
                                 ifo=args.ifo, segment_name=veto_segment_name)
    all_segments -= events.veto.select_segments_by_definer(veto_file,
                                                           ifo=args.ifo,
                                                           segment_name=veto_segment_name)
    stat = stat[retain]
    tid = tid[retain]
    time = time[retain]
    if args.save_trig_param:
        tparam = tparam[retain]
    logging.info('%i trigs left after vetoing with %s' %
                                                       (len(stat), veto_file))
total_time = abs(all_segments)

# do pruning (removal of trigs at N loudest times defined over param bins)
if args.prune_param:
    logging.info('Getting min and max param values')
    pars = triggers.get_param(args.prune_param, args,
                     templatef['mass1'][:], templatef['mass2'][:],
                     templatef['spin1z'][:], templatef['spin2z'][:])
    minpar = min(pars)
    maxpar = max(pars)
    del pars
    logging.info('%f %f' % (minpar, maxpar))

    # hard-coded time window of 0.1s
    args.prune_window = 0.1
    # initialize bin storage
    prunedtimes = {}
    for i in range(args.prune_bins):
        prunedtimes[i] = []

    # keep a record of the triggers if all successive loudest events were to
    # be pruned
    statpruneall = copy.deepcopy(stat)
    tidpruneall = copy.deepcopy(tid)
    timepruneall = copy.deepcopy(time)

    # many trials may be required to prune in 'quieter' bins
    for j in range(1000):
        # are all the bins full already?
        numpruned = sum([len(prunedtimes[i]) for i in range(args.prune_bins)])
        if numpruned == args.prune_bins * args.prune_number:
            logging.info('Finished pruning!')
            break
        if numpruned > args.prune_bins * args.prune_number:
            logging.error('Uh-oh, we pruned too many things .. %i, to be '
                          'precise' % numpruned)
            raise RuntimeError
        loudest = np.argmax(statpruneall)
        lstat = statpruneall[loudest]
        ltid = tidpruneall[loudest]
        ltime = timepruneall[loudest]
        m1, m2, s1z, s2z = triggers.get_mass_spin(templatef, ltid)
        lbin = trstats.which_bin(triggers.get_param(args.prune_param, args,
                                                    m1, m2, s1z, s2z),
                                 minpar, maxpar, args.prune_bins,
                                                      log=args.log_prune_param)
        # is the bin where the loudest trigger lives full already?
        if len(prunedtimes[lbin]) == args.prune_number:
            logging.info('%i - Bin %i full, not pruning event with stat %f at '
                         'time %.3f' % (j, lbin, lstat, ltime))
            # prune the reference trigger array
            retain = abs(timepruneall - ltime) > args.prune_window
            statpruneall = statpruneall[retain]
            tidpruneall = tidpruneall[retain]
            timepruneall = timepruneall[retain]
            del retain
            continue
        else:
            logging.info('Pruning event with stat %f at time %.3f in bin %i' %
                         (lstat, ltime, lbin))
            # now do the pruning
            retain = abs(time - ltime) > args.prune_window
            logging.info('%i trigs before pruning' % len(stat))
            stat = stat[retain]
            logging.info('%i trigs remain' % len(stat))
            tid = tid[retain]
            time = time[retain]
            if args.save_trig_param:
                tparam = tparam[retain]
            # also for the reference trig arrays
            retain = abs(timepruneall - ltime) > args.prune_window
            statpruneall = statpruneall[retain]
            tidpruneall = tidpruneall[retain]
            timepruneall = timepruneall[retain]
            # record the time
            prunedtimes[lbin].append(ltime)
            del retain
    del statpruneall
    del tidpruneall
    del timepruneall
    logging.info('%i trigs remain after pruning loop' % len(stat))

# parse template range
num_templates = len(templatef['template_hash'])
rangestr = args.template_fraction_range
part = int(rangestr.split('/')[0])
pieces = int(rangestr.split('/')[1])
tmin = int(num_templates / float(pieces) * part)
tmax = int(num_templates / float(pieces) * (part + 1))
trange = range(tmin, tmax)

logging.info('Analyzing template %s - %s' % (tmin, tmax-1))

trig_temp_id= trigf[args.ifo+'/template_id'][:]
ctemp=[]
ftemp=[]
matches=[]
for k in confile:
    ctemp.append(int(k))
    ftemp.append(confile[k+'/Fine_hashid'][...])
    matches.append(confile[k+'/vals/match'][...])

ctemp=np.asarray(ctemp)
ftemp=np.asarray(ftemp)
matches=np.asarray(matches)


def get_template_param(templateid):
    m1, m2, s1z, s2z = triggers.get_mass_spin(templatef, templateid)
    parvals = []
    for param in args.fit_param:
        parvals.append(triggers.get_param(param, args, m1, m2, s1z, s2z))
    return parvals

    
def get_alpha_from_stage1(i):
    tstart1=tt.time()
    # will return (coarse_template_id, position of ith fine template id in ftemp[x])
    #r=[(ctemp[cid],fid,np.where(ftemp[fid]==i)[0][0]) for cid, fid in zip(range(len(ctemp)),range(len(ftemp))) if len(np.where(ftemp[fid]==i)[0])!=0]
    #cids= np.array([r[x][0] for x in range(len(r))])
    #findex= np.array([r[x][1] for x in range(len(r))])
    #fpos= np.array([r[x][2] for x in range(len(r))])
    #mat= [matches[w][k] for w,k in zip(findex,fpos)]
    #c_cid=cids[np.argmax(mat)]
    r=[(ctemp[cid],fid,np.where(ftemp[fid]==i)[0][0]) for cid, fid in zip(range(len(ctemp)),range(len(ftemp))) if len(np.where(ftemp[fid]==i)[0])!=0]
    cids= np.array([r[x][0] for x in range(len(r))])
    findex= np.array([r[x][1] for x in range(len(r))])
    fpos= np.array([r[x][2] for x in range(len(r))])

    if fpos[0]==0 and len((matches[findex]))==1:
        mat=matches[findex][fpos]
        c_cid=cids[0]
    else:
        mat= [matches[w][k] for w,k in zip(findex,fpos)]
        c_cid=cids[np.argmax(mat)]

    template_duration_value,chieff_value,eta_value = get_template_param(i)
    return fit_stg1['fit_coeff'][:][c_cid],fit_stg1['median_sigma'][:][c_cid],fit_stg1['count_above_thresh'][:][c_cid],fit_stg1['count_in_template'][:][c_cid],float(template_duration_value),chieff_value,eta_value


# initialize result storage

tids = []
counts_total = []
counts_above = []
fits = []
tpars = []
etas=[]
chi_effs=[]
template_durations=[]
median_sigma=[]

#missing_templates=[i for i in range(num_templates) if i not in nbhds_template_ids]
#missing_templates=np.array([104641, 112541, 112564, 119240, 119260, 119310, 119311, 119521, 119524, 120918, 125330, 125331, 130337, 130437, 138406, 146201, 150423, 321424, 329453, 336824, 345776, 347393, 359289, 362246, 363114, 366672, 369681, 371904, 373539, 373540, 374228, 378019, 378556, 381072, 383076, 383077, 389228, 391008, 392585, 394785, 395017, 397562, 398715, 402014, 407439, 408807, 408808, 409353, 411772, 414241, 417497, 417531, 418471, 420621, 421058, 421899, 422352, 422933, 426792])



for tnum in trange:
    
    #stat_in_template = stat[tid == tnum]
    #print stat_in_template
    #count_above = len(stat_in_template)
    #count_total = count_in_template[tnum]
    print tnum
    #if tnum not in missing_templates:
    alpha,sig_alpha,count_above,count_total,template_duration,chi_eff,eta= get_alpha_from_stage1(tnum)
    #print alpha,sig_alpha,count_above,count_total,template_duration,chi_eff,eta

    tids.append(tnum)
    counts_above.append(count_above)
    counts_total.append(count_total)
    fits.append(alpha)
    etas.append(eta)
    chi_effs.append(chi_eff)
    template_durations.append(template_duration)
    median_sigma.append(sig_alpha)
    
    if args.save_trig_param:
        # save the param value of the first trig in this template
        param_in_template = tparam[tid == tnum]
        tpars.append(param_in_template[0])
    if (tnum % 1000 == 0): logging.info('Fitted template %i / %i' %
                                                    (tnum - tmin, tmax - tmin))


    
#logging.info("Calculating median sigma for each template")
#median_sigma = [np.sqrt(np.median(trigf[args.ifo + '/sigmasq'][reg])) for reg in
#                    trigf[args.ifo + '/sigmasq_template'][:]]

outfile = h5py.File(args.output, 'w')
# store template-dependent fit output
outfile.create_dataset("chi_eff", data=chi_effs)
outfile.create_dataset("count_above_thresh", data=counts_above)
outfile.create_dataset("count_in_template", data=counts_total)
outfile.create_dataset("eta", data=etas)
outfile.create_dataset("fit_coeff", data=fits)
outfile.create_dataset("median_sigma", data=median_sigma)
outfile.create_dataset("template_duration", data=template_durations)
outfile.create_dataset("template_id", data=trange)

if args.save_trig_param:
    outfile.create_dataset("template_param", data=tpars)

# add some metadata
outfile.attrs.create("ifo", data=args.ifo.encode())

#outfile.attrs.create("fit_function", data=args.fit_function.encode())
outfile.attrs.create("stat_threshold", data=args.sngl_stat.encode())

if args.save_trig_param:
    outfile.attrs.create("save_trig_param", data=args.save_trig_param.encode())
outfile.attrs.create("stat_threshold", data=args.stat_threshold)
outfile.attrs.create("analysis_time", data=total_time)

# add a magic file attribute so that coinc_findtrigs can parse it
outfile.attrs.create("stat",args.ifo.encode()+ b'-fit_coeffs')
outfile.close()
logging.info('Done!')



    






